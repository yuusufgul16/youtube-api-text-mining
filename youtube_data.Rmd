---
title: "R Notebook"
output: html_notebook
---
```{r}
#KullanacaÄŸÄ±mÄ±z paketleri yÃ¼klÃ¼yoruz
data("stop_words")
library(tuber)      #YouTube API ile etkileÅŸim kurmak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(httpuv)     #Elde ettiÄŸimiz Ã§Ä±ktÄ±larÄ± html olarak gÃ¶rmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(dplyr)      #metin verilerini temizlemek ve manipÃ¼le etmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(ggplot2)    #metin verilerini gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(tibble)     #metin verilerini temizlemek ve manipÃ¼le etmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(magrittr)   #%>% komutu ile zincirleme iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(pander)     #istatistiksel Ã¶zetler oluÅŸturmak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(pastecs)    #Ã§eÅŸitli istatistiksel yÃ¶ntemler ve Ã¶zetler sunar
library(sentimentr) #metin verilerinin duygusal yÃ¶nÃ¼nÃ¼ analiz etmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(stopwords)  #istenmeyen kelimeleri Ã§Ä±kartmak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(hwordcloud) #kelime bulutu oluÅŸturmak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(wordcloud2) #kelime bulutu oluÅŸturmak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(tidytext)   # metin verilerinden kelimeleri Ã§Ä±karmak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(stringr)    #metinlerle Ã§alÄ±ÅŸmayÄ± mÃ¼mkÃ¼n olduÄŸunca kolay hale getirmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
library(tm)         #metin verilerini iÅŸlemek ve analiz etmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
```
```{r}
#YouTube APIâ€™sine eriÅŸim saÄŸlamak iÃ§in tuber adlÄ± bir R paketini kullanÄ±yoruz "yt_oauth" fonksiyonu, YouTube APIâ€™sine eriÅŸim yetkisi saÄŸlar.
app_id <- "567072538173-pl92frocjar5146k2utecqvitm2ja7mk.apps.googleusercontent.com"
app_secret <- "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"
yt_oauth(app_id, app_secret, token =" ")
```
```{r}
#"get_all_comments" fonksiyonu, belirli bir YouTube videosunun tÃ¼m yorumlarÄ±nÄ± Ã§ekmek iÃ§in kullanÄ±ldÄ±.
video_ids<- c("SN2BZswEWUA","63yr9dlI0cU","kWmX3pd1f10","RzkD_rTEBYs","s0dMTAQM4cw","Sqa8Zo2XWc4","UwsrzCVZAb8")
comments_list <- lapply(video_ids, function(id) 
  get_all_comments(video_id = id))
all_comments <- do.call(rbind, comments_list)
```

```{r}
write.csv(all_comments, file = "all_comments.csv", row.names = FALSE)
```


```{r}
#"write.csv" fonksiyonu, bir veri Ã§erÃ§evesini (data frame) bir CSV dosyasÄ±na yazmak iÃ§in kullanÄ±lÄ±r. Bu durumda, comments adlÄ± veri Ã§erÃ§evemizi â€œyoutubecomments.csvâ€ adlÄ± bir dosyaya yazÄ±yoruz.

#read.csv fonksiyonu ise bir CSV dosyasÄ±nÄ± okuyarak bir veri Ã§erÃ§evesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Bu durumda, â€œyoutubecomments.csvâ€ adlÄ± dosyayÄ± okuyor ve iÃ§eriÄŸini datas adlÄ± bir veri Ã§erÃ§evesine atÄ±yoruz.

#Bu iki iÅŸlem, verilerimizi diske yazma ve daha sonra tekrar okuma iÅŸlemlerini gerÃ§ekleÅŸtirir. Bu, genellikle verilerimizi saklamak ve daha sonra tekrar kullanmak iÃ§in yapÄ±lÄ±r.
datas <- read.csv("all_comments.csv")

```

```{r}
datas <- read.csv("all_comments.csv")
```
```{r}
# Kendi durma kelimelerinizi ekleyin
custom_stop_words <- bind_rows(stop_words, data_frame(word = c("ts","rdj","tony","stark","video","overly", "lot", "day", "dyson", "sun", "lol","pan","bs","just","can","us","isnt", "",  "vğŸ…°ï¸nitus", "a","overly","donâ€™t","dont","iron","robert","youtube","itâ€™s", "stuff","yeah","youre","didnt","doesnt","fuck","shit","bullshit","ultron","fucking"  )))
```

```{r}
comments <- datas%>%
  dplyr::select(textOriginal)
```
```{r}
# Emojileri kaldÄ±rmak iÃ§in bir fonksiyon oluÅŸtur
remove_emoji <- function(x) {
  return(str_replace_all(x, "[<].*[>]", ""))
}
```
```{r}
                                               #TEMIZLIK ISLEMLERI
comment <- comments%>%
  mutate(word=str_to_lower(textOriginal))%>% unnest_tokens(word, textOriginal) # Metindeki harflerin tamami kucuk harfe donusturulmustur.
comment <-comment %>% mutate(word=removePunctuation(word)) # Metindeki noktalama iÅŸaretleri kaldÄ±rÄ±lmÄ±ÅŸtÄ±r.
comment <-comment  %>% mutate(word=str_squish(word)) # Metinle iÃ§ iÃ§e geÃ§miÅŸ (Ã¶rnek:yusuf123) rakam ve sayÄ±lar metinden ayÄ±klanmÄ±ÅŸtÄ±r.
comment <-comment  %>% mutate(word=removeNumbers(word)) # Metinden rakam ve sayÄ±lar Ã§Ä±karÄ±lmÄ±ÅŸtÄ±r.
uzatilmis_kelime_deseni <- "([a-z])\\1{2,}" # UzatÄ±lmÄ±ÅŸ kelimeleri bul
comment$word <- str_replace_all(comment$word, uzatilmis_kelime_deseni, "\\1")# UzatÄ±lmÄ±ÅŸ kelimeleri kaldÄ±r
comment <- comment %>% mutate(word = sapply(word, remove_emoji))#emoji temizle
comment <- comment %>%
  mutate(word = str_replace_all(word, "[<].*[>]", ""),#HTML etiketlerini kaldÄ±rmak iÃ§in kullanÄ±lÄ±r.
         word = gsub("\uFFFD", "", word, fixed = TRUE),#\uFFFD genellikle bilinmeyen veya tanÄ±mlanamayan karakterleri temsil eder.
         word = gsub("\n", "", word, fixed = TRUE))#her bir kelimenin iÃ§indeki yeni satÄ±r karakterlerini (\n) siler.

comment <- comment %>% filter(!word %in% custom_stop_words$word) # Durma kelimelerini kaldÄ±r
comment <-comment %>% filter(str_length(word)>3) # Karakter sayÄ±sÄ± 3'ten bÃ¼yÃ¼k kelimeler filtrelenmiÅŸtir. 
comment <-str_replace(comment$word, "[Ä±]", "i") # Metin istenmeyen formatta kelimeler varsa ve bu kelimeleri Ã§Ä±karmanÄ±z analize zarar verecekse yerine kelimeler atayabilirsiniz.
comment <-comment %>% as_tibble()%>%rename(word=value) # Metnin deÄŸiÅŸiklikler sonra tibble tablo dÃ¼zenine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.
```
```{r}
# ingilizce olan kelimeler alÄ±ndÄ±.
comment <- filter(comment, str_detect(word, "[a-zA-Z]+"))
```

```{r}
words <- comment %>%
  unnest_tokens(word, word) %>%
  anti_join(custom_stop_words) %>%
  count(word, sort = TRUE)
```
```{r}
words%>%#ggplot2 kÃ¼tÃ¼phanesinden yararlanarak yatay bir biÃ§imde ilk 20 verimizi gÃ¶rselleÅŸtirdik
  head(20)%>%
  ggplot(aes(reorder(word, n),n))+
  geom_col(fill="#5c9aab", alpha=0.6)+
  coord_flip()+
  labs(x="Kelimeler",
       y="Tekrar Edilme SayÄ±sÄ±",
       title = "EN Ã‡OK TEKRAR EDÄ°LEN 20 KELÄ°ME")+
  theme_test()
```
```{r}

wordcloud2(data = head(words, 100), 
           minRotation = -pi/6, 
           maxRotation = -pi/3, 
           minSize = 0.1, 
           rotateRatio = 0, 
           size = .7, 
           color = "random-dark", 
           fontWeight = "bold", 
           fontFamily = "sans")
```
```{r}
# Bing duygu sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ alÄ±n
bing_sentiments <- get_sentiments("bing")

# Kelimelerinizi duygu deÄŸerleriyle birleÅŸtirin
words_sentiments <- words %>%
  inner_join(bing_sentiments, by = "word")

# Pozitif ve negatif kelimeleri ayÄ±rÄ±n
positive_words <- words_sentiments %>%
  filter(sentiment == "positive") %>%
  head(100)

negative_words <- words_sentiments %>%
  filter(sentiment == "negative") %>%
  head(100)
```

```{r}
wordcloud2(positive_words)
```

```{r}
wordcloud2(negative_words)
```


```{r}
words %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(sentiment) %>%
  summarise(n = mean(n)) %>%
  ggplot(aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  scale_fill_manual(values = c("positive" = "#00bfc4", "negative" = "#f8766d"))+
  xlab("Duygu") +
  ylab("Frekans")
```


```{r}
words %>%
  inner_join(get_sentiments("bing")) %>%
  head(20) %>%
  arrange(-n) %>%
  group_by(sentiment) %>%
  ggplot(aes(reorder(word, n), n, color = sentiment)) +
  geom_line(aes(group = 1)) +
  geom_point() +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  xlab("Kelime") +
  ylab("Frekans")

```
```{r}
polarite<-sentiment(comment$word)
stat.desc(polarite$sentiment, basic=T) %>% pander()
```
```{r}
tablo<-cbind(words$word, polarite[,c(3,4)])
```

```{r}


ggplot(tablo, aes(word_count, sentiment)) +
  geom_point(color="#5c9aab") +
  geom_hline(yintercept = mean(tablo$sentiment), color="red", size=.5) +
  labs(y = "Skor", x = "Count") +
  theme(plot.caption = element_text(hjust = 0, face = "italic"))
```


